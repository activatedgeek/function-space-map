{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this once\n",
    "!git clone https://github.com/hughsalimbeni/bayesian_benchmarks.git\n",
    "!mv bayesian_benchmarks/bayesian_benchmarks tmp\n",
    "!rm -rf bayesian_benchmarks\n",
    "!mv tmp bayesian_benchmarks\n",
    "!pip install xlrd openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax backend gpu\n",
      "jax devices [cuda(id=0)]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "print('jax backend {}'.format(jax.lib.xla_bridge.get_backend().platform))\n",
    "print('jax devices {}'.format(jax.devices()))\n",
    "\n",
    "\n",
    "from bayesian_benchmarks.data import get_regression_data, get_classification_data\n",
    "import bayesian_benchmarks.data as bd\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "from flax.training import train_state\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import linen as nn\n",
    "from typing import Callable\n",
    "\n",
    "def count_params(model, x):\n",
    "    params = model.init(jax.random.PRNGKey(0), x)\n",
    "    n = sum([np.prod(p.shape) for p in jax.tree_util.tree_leaves(params)])\n",
    "    print(f'Number of parameters: {n/1e3} k')\n",
    "    \n",
    "def param_size(params):\n",
    "    return sum([np.prod(p.shape) for p in jax.tree_util.tree_leaves(params)])\n",
    "\n",
    "def tree_norm(tree):\n",
    "    return jnp.sqrt(sum([jnp.sum(x**2) for x in jax.tree_leaves(tree)]))\n",
    "\n",
    "def random_split_like_tree(rng_key, target=None, treedef=None):\n",
    "    if treedef is None:\n",
    "        treedef = jax.tree_structure(target)\n",
    "    keys = jax.random.split(rng_key, treedef.num_leaves)\n",
    "    return jax.tree_unflatten(treedef, keys)\n",
    "\n",
    "@jax.jit\n",
    "def tree_random_normal_like(rng_key, target):\n",
    "    keys_tree = random_split_like_tree(rng_key, target)\n",
    "    return jax.tree_map(\n",
    "        lambda l, k: jax.random.normal(k, l.shape, l.dtype),\n",
    "        target,\n",
    "        keys_tree,\n",
    "    )\n",
    "    \n",
    "@partial(jax.jit, static_argnums=(0,))\n",
    "def laplacian_estimator(model, p, x, dp, sigma=1e-3):\n",
    "    # dp: N(0, I)\n",
    "    # p_perturbed = p + dp, both pytrees\n",
    "    # scale dp by sigma\n",
    "    dp = jax.tree_util.tree_map(lambda x: x * sigma, dp)\n",
    "    p_perturbed = jax.tree_util.tree_map(lambda x, y: x + y, p, dp)\n",
    "    # compute the difference between the outputs\n",
    "    y = model.apply(p, x)\n",
    "    y_perturbed = model.apply(p_perturbed, x)\n",
    "    diff = y - y_perturbed\n",
    "    # compute the avg sq norm of the difference\n",
    "    avg_dff_sq_norm = jnp.mean((diff ** 2).sum(axis=-1))\n",
    "    return avg_dff_sq_norm / (sigma ** 2)\n",
    "\n",
    "def standardize(x_train, x_test):\n",
    "    assert x_train.ndim == 2 and x_test.ndim == 2 and x_train.shape[1] == x_test.shape[1], 'x_train and x_test should have the same number of features'\n",
    "    x_train = (x_train - x_train.mean(0)) / (x_train.std(0) + 1e-8)\n",
    "    x_test = (x_test - x_train.mean(0)) / (x_train.std(0) + 1e-8)\n",
    "    return x_train, x_test\n",
    "\n",
    "def standardize_ds(x_train, y_train, x_test, y_test, is_classification):\n",
    "    # X: (N, D)\n",
    "    # Y: (N, Dy)\n",
    "    x_train = x_train.astype(np.float32)\n",
    "    x_test = x_test.astype(np.float32)\n",
    "    x_train, x_test = standardize(x_train, x_test)\n",
    "    if is_classification:\n",
    "        y_train = y_train.astype(np.int32)\n",
    "        y_test = y_test.astype(np.int32)\n",
    "    else:\n",
    "        y_train = y_train.astype(np.float32)\n",
    "        y_test = y_test.astype(np.float32)\n",
    "        y_train, y_test = standardize(y_train, y_test)\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    out_size: int\n",
    "    H: int = 64\n",
    "    hidden_layers: int = 1\n",
    "    act: Callable = nn.relu\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        layers = [nn.Dense(self.H)]\n",
    "        for _ in range(self.hidden_layers - 1):\n",
    "            layers += [self.act, nn.Dense(self.H)]\n",
    "        layers += [self.act, nn.Dense(self.out_size)]\n",
    "        return nn.Sequential(layers)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(train_loss_fn, test_loss_fn, neg_log_p_w_fn, n_train, init_params, lr, n_step, rng_key, model, x_eval_generator, optimizer, method, laplace_scale=1.0):    \n",
    "    def get_train_state(optimizer, lr, init_params, warmup_steps=100):\n",
    "        if optimizer == 'adam':\n",
    "            tx = optax.chain(\n",
    "                    optax.adam(learning_rate=lr),\n",
    "                )\n",
    "        elif optimizer == 'sgd':\n",
    "            tx = optax.sgd(learning_rate=lr, momentum=0.9)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return train_state.TrainState.create(apply_fn=model.apply, params=init_params, tx=tx)\n",
    "    \n",
    "    ts = get_train_state(optimizer, lr, init_params)\n",
    "    \n",
    "    def augmented_loss_fn(p, x_eval, dp, rng_key):\n",
    "        # loss = likelihood / n_train = 1 / (2 * sigma^2) * ||y - f(x)||^2 / n_train\n",
    "        # it contains a factor 1 / n_train\n",
    "        # all other terms should be divided by n_train as well\n",
    "        avg_nll = train_loss_fn(p, rng_key) \n",
    "        test_loss = test_loss_fn(p)\n",
    "        # number of params\n",
    "        P = jax.tree_util.tree_leaves(p)[0].shape[0]\n",
    "        if method == 'lmap':\n",
    "            logdet = laplacian_estimator(model, p, x_eval, dp)\n",
    "        elif method == 'psmap':\n",
    "            logdet = 0\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        neg_log_p_w = neg_log_p_w_fn(p)\n",
    "        logdet =  laplace_scale * logdet + (1 - laplace_scale) * jax.lax.stop_gradient(logdet)\n",
    "        # return avg_nll + neg_log_p_w + logdet, (avg_nll, logdet, test_loss)\n",
    "        return avg_nll + neg_log_p_w + logdet, (avg_nll, logdet, test_loss)\n",
    "\n",
    "    grad_fn = jax.jit(jax.value_and_grad(lambda p, x_eval, dp, batch_key: augmented_loss_fn(p, x_eval, dp, batch_key), has_aux=True))\n",
    "    losses = []\n",
    "    logdets = []\n",
    "    avg_nlls = []\n",
    "    test_losss = []\n",
    "    def sample_dp(p, rng_key):\n",
    "        # gaussian tree\n",
    "        dp = tree_random_normal_like(rng_key, p)\n",
    "        return dp\n",
    "    @jax.jit\n",
    "    def train_step(ts, rng_key):\n",
    "        rng_key, x_eval_key = jax.random.split(rng_key)\n",
    "        x_eval = x_eval_generator(x_eval_key)\n",
    "        rng_key, dp_key = jax.random.split(rng_key)\n",
    "        dp = sample_dp(ts.params, dp_key)\n",
    "        rng_key, batch_key = jax.random.split(rng_key)\n",
    "        (loss, aux), grads = grad_fn(ts.params, x_eval, dp, batch_key)\n",
    "        ts = ts.apply_gradients(grads=grads)\n",
    "        return ts, loss, aux, rng_key\n",
    "    for _ in tqdm(range(int(n_step))):\n",
    "        ts, loss, aux, rng_key = train_step(ts, rng_key)\n",
    "        avg_nll, logdet, test_loss = aux\n",
    "        losses.append(loss.item())\n",
    "        logdets.append(logdet.item())\n",
    "        avg_nlls.append(avg_nll.item())\n",
    "        test_losss.append(test_loss.item())\n",
    "    losses = np.array(losses)\n",
    "    logdets = np.array(logdets)\n",
    "    avg_nlls = np.array(avg_nlls)\n",
    "    test_losss = np.array(test_losss)\n",
    "    \n",
    "    return ts.params, losses, avg_nlls, logdets, test_losss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def to_one_hot(y_train, y_val, y_test):\n",
    "    # y_train: (N,1)\n",
    "    # y_test: (N,1)\n",
    "    assert y_train.ndim == 2 and y_test.ndim == 2 and y_train.shape[1] == y_test.shape[1] == 1, 'y_train and y_test should have shape (N,1)'\n",
    "    n_classes = y_train.max() + 1\n",
    "    y_train = jax.nn.one_hot(y_train.reshape(-1), n_classes)\n",
    "    y_val = jax.nn.one_hot(y_val.reshape(-1), n_classes)\n",
    "    y_test = jax.nn.one_hot(y_test.reshape(-1), n_classes)\n",
    "    return y_train, y_val, y_test\n",
    "\n",
    "def run_dataset(dataset, arch, wd, n_step, batch_size, lr, optimizer, output_dir, seed, method, laplace_scale, p_X='noise', result_path=None):\n",
    "    is_classification = dataset in bd.classification_datasets\n",
    "    if is_classification:\n",
    "        print(f'Running {dataset} classification dataset')\n",
    "    else:\n",
    "        print(f'Running {dataset} regression dataset')\n",
    "    # get data\n",
    "    ds = get_regression_data(dataset) if not is_classification else get_classification_data(dataset)\n",
    "    x_train, y_train, x_test, y_test = ds.X_train, ds.Y_train, ds.X_test, ds.Y_test\n",
    "    if seed != 0:\n",
    "        # shuffle train and test since UCI has high variance across splits\n",
    "        x_combined = np.concatenate([x_train, x_test], axis=0)\n",
    "        y_combined = np.concatenate([y_train, y_test], axis=0)\n",
    "        indices = np.arange(x_combined.shape[0])\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(indices)\n",
    "        x_combined = x_combined[indices]\n",
    "        y_combined = y_combined[indices]\n",
    "        x_train, x_test = x_combined[:x_train.shape[0]], x_combined[x_train.shape[0]:]\n",
    "        y_train, y_test = y_combined[:y_train.shape[0]], y_combined[y_train.shape[0]:]\n",
    "        \n",
    "    x_train, y_train, x_test, y_test = standardize_ds(x_train, y_train, x_test, y_test, is_classification)\n",
    "    # 10% of training data as validation\n",
    "    n_val = int(x_train.shape[0] * 0.1)\n",
    "    # shuffle x_train and y_train\n",
    "    indices = np.arange(x_train.shape[0])\n",
    "    # fix seed\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(indices)\n",
    "    x_train = jnp.asarray(x_train[indices])\n",
    "    y_train = jnp.asarray(y_train[indices])\n",
    "    x_train, x_val, y_train, y_val = x_train[n_val:], x_train[:n_val], y_train[n_val:], y_train[:n_val]\n",
    "    if is_classification:\n",
    "        y_train, y_val, y_test = to_one_hot(y_train, y_val, y_test)\n",
    "    print(f\"X train: {x_train.shape}, X val: {x_val.shape}, X test: {x_test.shape}\")\n",
    "    print(f\"Y train: {y_train.shape}, Y val: {y_val.shape}, Y test: {y_test.shape}\")\n",
    "    # model\n",
    "    model = arch(out_size=y_train.shape[1])\n",
    "    def x_eval_generator(rng_key):\n",
    "        # sample batch_size points from training set\n",
    "        if p_X == 'train':\n",
    "            ind = jax.random.randint(rng_key, (batch_size,), 0, x_train.shape[0])\n",
    "            return x_train[ind]\n",
    "        elif p_X == 'noise':\n",
    "            return jax.random.normal(rng_key, (batch_size, x_train.shape[1]))\n",
    "        else:\n",
    "            raise ValueError(f'p_X should be either train or noise, got {p_X}')\n",
    "        \n",
    "    # count parameters\n",
    "    init_params = model.init(jax.random.PRNGKey(0), jnp.ones((1, x_train.shape[1])))\n",
    "    leaves, _ = jax.tree_util.tree_flatten(init_params)\n",
    "    n_params = sum([np.prod(p.shape) for p in leaves])\n",
    "    print(f\"Number of parameters: {n_params}\")\n",
    "\n",
    "    # softmax multi-task cross entropy loss (not binary)\n",
    "    if is_classification:\n",
    "        loss_fn = lambda p, x, y: jnp.mean(jnp.sum(-jax.nn.log_softmax(model.apply(p, x), axis=1) * y, axis=1))\n",
    "    else:\n",
    "        loss_fn = lambda p, x, y: jnp.mean((model.apply(p, x) - y)**2)\n",
    "    y_model = model.apply(init_params, x_test)\n",
    "    assert y_model.shape == y_test.shape, f'model output shape: {y_model.shape}, y_test shape: {y_test.shape}'\n",
    "    def train_loss_fn(p, rng_key):\n",
    "        ind = jax.random.randint(rng_key, (batch_size,), 0, x_train.shape[0])\n",
    "        return loss_fn(p, x_train[ind], y_train[ind])\n",
    "        # return loss_fn(p, x_train, y_train)\n",
    "    test_loss_fn = lambda p: loss_fn(p, x_test, y_test)\n",
    "    val_loss_fn = lambda p: loss_fn(p, x_val, y_val)\n",
    "    \n",
    "    rng_key = jax.random.PRNGKey(seed)\n",
    "    # log training time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # params, losses, avg_losses, val_losses, logdets = optimize(lr, prior_scale, n_step, rng_key, train_loss_fn, val_loss_fn, model, x_train, y_train, x_eval_generator, optimizer, fsmap, jitter, method, laplace_scale)\n",
    "    \n",
    "    neg_log_p_w_fn = lambda p: jnp.sum((jax.flatten_util.ravel_pytree(p)[0] ** 2)) * wd\n",
    "    n_train = x_train.shape[0]\n",
    "    params, losses, avg_losses, logdets, test_losses = optimize(train_loss_fn, test_loss_fn, neg_log_p_w_fn, n_train, init_params, lr, n_step, rng_key, model, x_eval_generator, optimizer, method, laplace_scale)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    # train time in seconds\n",
    "    train_time = end_time - start_time\n",
    "    # plot and save losses\n",
    "    plt.figure()\n",
    "    \n",
    "    def make_subplots(arrays, labels):\n",
    "        n_plots = len(arrays)\n",
    "        fig, axs = plt.subplots(n_plots, 1, figsize=(5, 5))\n",
    "        for i, (a, label) in enumerate(zip(arrays, labels)):\n",
    "            q1, q2 = np.quantile(a, [0., 1])\n",
    "            axs[i].plot(a)\n",
    "            axs[i].set_ylim(q1, q2)\n",
    "            axs[i].set_xlabel('Step')\n",
    "            axs[i].set_ylabel(label)\n",
    "            axs[i].legend()\n",
    "        plt.show()\n",
    "    \n",
    "    make_subplots([avg_losses, test_losses, logdets], ['Train Loss', 'Test Loss', 'Logdet'])\n",
    "    \n",
    "    \n",
    "    train_rmse = lambda p: jnp.sqrt(jnp.mean((model.apply(p, x_train) - y_train) ** 2))\n",
    "    val_rmse = lambda p: jnp.sqrt(jnp.mean((model.apply(p, x_val) - y_val) ** 2))\n",
    "    test_rmse = lambda p: jnp.sqrt(jnp.mean((model.apply(p, x_test) - y_test) ** 2))\n",
    "    \n",
    "    def measure(params):\n",
    "        measurements = {\n",
    "            'train_rmse': train_rmse(params).item(),\n",
    "            'val_rmse': val_rmse(params).item(),\n",
    "            'test_rmse': test_rmse(params).item(),\n",
    "            'train_time': train_time,\n",
    "            'laplace_scale': laplace_scale,\n",
    "            'wd': wd,\n",
    "            'dataset': dataset,\n",
    "        }\n",
    "        print(measurements)\n",
    "        # return log_likelihood + neg_log_param_prior - 1 / 2 * log_det\n",
    "        return measurements\n",
    "    \n",
    "    measurements = measure(params)\n",
    "    torch.save(measurements, result_path)\n",
    "\n",
    "    return measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded result from  uci/psmap/boston_mlp_wd9.999999999999999e-06.seed0.pt\n",
      "Loaded result from  uci/psmap/boston_mlp_wd9.999999999999999e-05.seed0.pt\n",
      "Loaded result from  uci/psmap/boston_mlp_wd0.001.seed0.pt\n",
      "Loaded result from  uci/psmap/boston_mlp_wd0.01.seed0.pt\n",
      "Loaded result from  uci/psmap/boston_mlp_wd0.09999999999999999.seed0.pt\n",
      "Loaded result from  uci/psmap/concrete_mlp_wd9.999999999999999e-06.seed0.pt\n",
      "Loaded result from  uci/psmap/concrete_mlp_wd9.999999999999999e-05.seed0.pt\n",
      "Loaded result from  uci/psmap/concrete_mlp_wd0.001.seed0.pt\n",
      "Loaded result from  uci/psmap/concrete_mlp_wd0.01.seed0.pt\n",
      "Loaded result from  uci/psmap/concrete_mlp_wd0.09999999999999999.seed0.pt\n",
      "Loaded result from  uci/psmap/energy_mlp_wd9.999999999999999e-06.seed0.pt\n",
      "Loaded result from  uci/psmap/energy_mlp_wd9.999999999999999e-05.seed0.pt\n",
      "Loaded result from  uci/psmap/energy_mlp_wd0.001.seed0.pt\n",
      "Loaded result from  uci/psmap/energy_mlp_wd0.01.seed0.pt\n",
      "Loaded result from  uci/psmap/energy_mlp_wd0.09999999999999999.seed0.pt\n",
      "Loaded result from  uci/psmap/power_mlp_wd9.999999999999999e-06.seed0.pt\n",
      "Loaded result from  uci/psmap/power_mlp_wd9.999999999999999e-05.seed0.pt\n",
      "Loaded result from  uci/psmap/power_mlp_wd0.001.seed0.pt\n",
      "Loaded result from  uci/psmap/power_mlp_wd0.01.seed0.pt\n",
      "Loaded result from  uci/psmap/power_mlp_wd0.09999999999999999.seed0.pt\n",
      "Loaded result from  uci/psmap/protein_mlp_wd9.999999999999999e-06.seed0.pt\n",
      "Loaded result from  uci/psmap/protein_mlp_wd9.999999999999999e-05.seed0.pt\n",
      "Loaded result from  uci/psmap/protein_mlp_wd0.001.seed0.pt\n",
      "Loaded result from  uci/psmap/protein_mlp_wd0.01.seed0.pt\n",
      "Loaded result from  uci/psmap/protein_mlp_wd0.09999999999999999.seed0.pt\n",
      "Loaded result from  uci/psmap/winered_mlp_wd9.999999999999999e-06.seed0.pt\n",
      "Loaded result from  uci/psmap/winered_mlp_wd9.999999999999999e-05.seed0.pt\n",
      "Loaded result from  uci/psmap/winered_mlp_wd0.001.seed0.pt\n",
      "Loaded result from  uci/psmap/winered_mlp_wd0.01.seed0.pt\n",
      "Loaded result from  uci/psmap/winered_mlp_wd0.09999999999999999.seed0.pt\n",
      "Loaded result from  uci/psmap/winewhite_mlp_wd9.999999999999999e-06.seed0.pt\n",
      "Loaded result from  uci/psmap/winewhite_mlp_wd9.999999999999999e-05.seed0.pt\n",
      "Loaded result from  uci/psmap/winewhite_mlp_wd0.001.seed0.pt\n",
      "Loaded result from  uci/psmap/winewhite_mlp_wd0.01.seed0.pt\n",
      "Loaded result from  uci/psmap/winewhite_mlp_wd0.09999999999999999.seed0.pt\n",
      "Loaded result from  uci/psmap/naval_mlp_wd9.999999999999999e-06.seed0.pt\n",
      "Loaded result from  uci/psmap/naval_mlp_wd9.999999999999999e-05.seed0.pt\n",
      "Loaded result from  uci/psmap/naval_mlp_wd0.001.seed0.pt\n",
      "Loaded result from  uci/psmap/naval_mlp_wd0.01.seed0.pt\n",
      "Loaded result from  uci/psmap/naval_mlp_wd0.09999999999999999.seed0.pt\n",
      "Best wd for boston is 0.01\n",
      "Best wd for concrete is 9.999999999999999e-05\n",
      "Best wd for energy is 9.999999999999999e-05\n",
      "Running energy regression dataset\n",
      "X train: (622, 8), X val: (69, 8), X test: (77, 8)\n",
      "Y train: (622, 1), Y val: (69, 1), Y test: (77, 1)\n"
     ]
    }
   ],
   "source": [
    "# PSMAP\n",
    "\n",
    "width = 256\n",
    "depth = 3\n",
    "\n",
    "\n",
    "optimizer = 'adam'\n",
    "lr = 1e-3\n",
    "n_step = int(1e4)\n",
    "batch_size = 512\n",
    "wds = np.logspace(-5, -1, 5)\n",
    "\n",
    "\n",
    "skip_if_done = True\n",
    "method = 'psmap'\n",
    "\n",
    "datasets = ['boston', 'concrete', 'energy', 'power', 'protein', 'winered', 'winewhite', 'naval']\n",
    "\n",
    "# tune weight decay (i.e. prior scale)\n",
    "psmap_results = []\n",
    "output_dir = f'uci/{method}'\n",
    "# mkdir if needed\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "for dataset in datasets:\n",
    "    for wd in wds:\n",
    "        for seed in range(1):\n",
    "            arch = partial(MLP, H=width, hidden_layers=depth)\n",
    "            result_path = f'{output_dir}/{dataset}_mlp_wd{wd}.seed{seed}.pt'\n",
    "            if os.path.exists(result_path) and skip_if_done:\n",
    "                psmap_results.append(torch.load(result_path))\n",
    "                print('Loaded result from ', result_path)\n",
    "            else:\n",
    "                psmap_results.append(run_dataset(dataset, arch, wd, n_step, batch_size, lr, optimizer, output_dir, seed, method=method, laplace_scale=0, result_path=result_path))\n",
    "\n",
    "# run with best wd\n",
    "output_dir = f'uci/{method}'\n",
    "psmap_results = []\n",
    "for dataset in datasets:\n",
    "    best_wd = None\n",
    "    best_val_rmse = float('inf')\n",
    "    for wd in wds:\n",
    "        result_path = f'{output_dir}/{dataset}_mlp_wd{wd}.seed0.pt'\n",
    "        val_rmse = torch.load(result_path)['val_rmse']\n",
    "        if val_rmse < best_val_rmse:\n",
    "            best_val_rmse = val_rmse\n",
    "            best_wd = wd\n",
    "    print(f'Best wd for {dataset} is {best_wd}')\n",
    "    arch = partial(MLP, H=width, hidden_layers=depth)\n",
    "    for seed in range(6):\n",
    "        result_path = f'{output_dir}/{dataset}_mlp_best.seed{seed}_shuffled.pt'\n",
    "        if os.path.exists(result_path) and skip_if_done:\n",
    "            psmap_results.append(torch.load(result_path))\n",
    "        else:\n",
    "            psmap_results.append(run_dataset(dataset, arch, best_wd, n_step, batch_size, lr, optimizer, output_dir, seed, method=method, laplace_scale=0, result_path=result_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LMAP\n",
    "\n",
    "width = 256\n",
    "depth = 3\n",
    "\n",
    "\n",
    "optimizer = 'adam'\n",
    "lr = 1e-3\n",
    "n_step = int(1e4)\n",
    "batch_size = 512\n",
    "wds = np.logspace(-5, -1, 5)\n",
    "laplace_scales = np.logspace(-5, -1, 5)\n",
    "\n",
    "\n",
    "skip_if_done = True\n",
    "method = 'lmap'\n",
    "\n",
    "datasets = ['boston', 'concrete', 'energy', 'power', 'protein', 'winered', 'winewhite', 'naval']\n",
    "\n",
    "# tune weight decay (i.e. prior scale) and laplace scale\n",
    "lmap_results = []\n",
    "output_dir = f'uci/{method}'\n",
    "# mkdir if needed\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "for dataset in datasets:\n",
    "    for wd in wds:\n",
    "        for laplace_scale in laplace_scales:\n",
    "            for seed in range(1):\n",
    "                arch = partial(MLP, H=width, hidden_layers=depth)\n",
    "                result_path = f'{output_dir}/{dataset}_mlp_pXnoise.l{laplace_scale}.wd{wd}.seed{seed}.pt'\n",
    "                if os.path.exists(result_path) and skip_if_done:\n",
    "                    lmap_results.append(torch.load(result_path))\n",
    "                    print('Loaded result from ', result_path)\n",
    "                else:\n",
    "                    gt_params = None\n",
    "                    lmap_results.append(run_dataset(dataset, arch, wd, n_step, batch_size, lr, optimizer, output_dir, seed, method=method, laplace_scale=laplace_scale, result_path=result_path))\n",
    "\n",
    "# run with best wd and laplace scale      \n",
    "output_dir = f'uci/{method}'                    \n",
    "lmap_results = []\n",
    "for dataset in datasets:\n",
    "    best_wd = None\n",
    "    best_laplace_scale = None\n",
    "    best_val_rmse = float('inf')\n",
    "    for wd in wds:\n",
    "        for laplace_scale in laplace_scales:\n",
    "            result_path = f'{output_dir}/{dataset}_mlp_pXnoise.l{laplace_scale}.wd{wd}.seed0.pt'\n",
    "            val_rmse = torch.load(result_path)['val_rmse']\n",
    "            if val_rmse < best_val_rmse:\n",
    "                best_val_rmse = val_rmse\n",
    "                best_wd = wd\n",
    "                best_laplace_scale = laplace_scale\n",
    "    print(f'Best wd, laplace_scale for {dataset} is ({best_wd}, {best_laplace_scale})')\n",
    "    arch = partial(MLP, H=width, hidden_layers=depth)\n",
    "    for seed in range(6):\n",
    "        result_path = f'{output_dir}/{dataset}_mlp_best.seed{seed}_shuffled.pt'\n",
    "        if os.path.exists(result_path) and skip_if_done:\n",
    "            lmap_results.append(torch.load(result_path))\n",
    "        else:\n",
    "            lmap_results.append(run_dataset(dataset, arch, best_wd, n_step, batch_size, lr, optimizer, output_dir, seed, method=method, laplace_scale=best_laplace_scale, result_path=result_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fsmap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
