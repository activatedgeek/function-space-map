{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "os.environ['OMP_NUM_THREADS'] = '4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import torch\n",
    "import time\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"paper\", font_scale=3, rc={\"lines.linewidth\": 2})\n",
    "sns.set_palette(\"Set1\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import set_matplotlib_formats\n",
    "\n",
    "set_matplotlib_formats(\"pdf\", \"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import linen as nn\n",
    "from typing import Callable, List\n",
    "\n",
    "def count_params(model, x):\n",
    "    params = model.init(jax.random.PRNGKey(0), x)\n",
    "    n = sum([np.prod(p.shape) for p in jax.tree_util.tree_leaves(params)])\n",
    "    print(f'Number of parameters: {n/1e3} k')\n",
    "    \n",
    "def param_size(params):\n",
    "    return sum([np.prod(p.shape) for p in jax.tree_util.tree_leaves(params)])\n",
    "\n",
    "class NonLinearMixture(nn.Module):\n",
    "    feature_fn: Callable\n",
    "    act_fn: Callable\n",
    "    prior_stds: List[float]\n",
    "    is_orthogonal: bool = False\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # evaluate basis functions\n",
    "        features = self.feature_fn(x) # (n, d)\n",
    "        # sample from N(0, prior_stds ** 2)\n",
    "        mixture_coeffs = self.param('mixture_coeffs', lambda key: jax.random.normal(key, (features.shape[-1],)) * self.prior_stds)\n",
    "        mixture_coeffs = self.act_fn(mixture_coeffs) / jnp.sqrt(mixture_coeffs.shape[-1]) # (d, )\n",
    "        return (features @ mixture_coeffs).reshape(-1, 1) # (n, 1)\n",
    "    \n",
    "class LogDet:\n",
    "    def __init__(self, model: NonLinearMixture, x_eval: jnp.ndarray, svd=False):\n",
    "        features = model.feature_fn(x_eval) # (n, d)\n",
    "        self.Phi = features.T @ features / features.shape[0] # (d, d)\n",
    "        self.act_deriv = jax.grad(model.act_fn)\n",
    "        self.act_deriv = jax.vmap(self.act_deriv)\n",
    "        self.diag = model.is_orthogonal\n",
    "        self.svd = svd\n",
    "        \n",
    "    \n",
    "    def compute_log_det(self, model_params, jitter=0, return_J=False):\n",
    "        # J[i,j] = s(w_i) * s(w_j) * Phi[i,j], where s is the derivative of model.act_fn\n",
    "        s = self.act_deriv(model_params['params']['mixture_coeffs'])\n",
    "        if self.diag:\n",
    "            print('DIAG!')\n",
    "            Phi_diag = jnp.diag(self.Phi)\n",
    "            return jnp.sum(jnp.log(Phi_diag * (s ** 2) + jitter))\n",
    "        J = s[:, None] * s[None, :] * self.Phi # (d, d)\n",
    "        # compute with svd\n",
    "        if self.svd:\n",
    "            singular_values = jnp.linalg.svd(J, compute_uv=False)\n",
    "            eigs = singular_values ** 2 + jitter\n",
    "            logdet = jnp.sum(jnp.log(eigs))\n",
    "        else:\n",
    "            logdet = jnp.linalg.slogdet(J + jitter * jnp.eye(J.shape[0]))[1]\n",
    "        if return_J:\n",
    "            return logdet, J\n",
    "        return logdet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_norm(tree):\n",
    "    return jnp.sqrt(sum([jnp.sum(x**2) for x in jax.tree_leaves(tree)]))\n",
    "\n",
    "def random_split_like_tree(rng_key, target=None, treedef=None):\n",
    "    if treedef is None:\n",
    "        treedef = jax.tree_structure(target)\n",
    "    keys = jax.random.split(rng_key, treedef.num_leaves)\n",
    "    return jax.tree_unflatten(treedef, keys)\n",
    "\n",
    "@jax.jit\n",
    "def tree_random_normal_like(rng_key, target):\n",
    "    keys_tree = random_split_like_tree(rng_key, target)\n",
    "    return jax.tree_map(\n",
    "        lambda l, k: jax.random.normal(k, l.shape, l.dtype),\n",
    "        target,\n",
    "        keys_tree,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(train_loss_fn, test_loss_fn, neg_log_p_w_fn, n_train, init_params, lr, n_step, rng_key, model, x_eval_generator, logdet_calculator, optimizer, jitter, method, temp=1.0):    \n",
    "    def get_train_state(optimizer, lr, init_params, warmup_steps=100):\n",
    "        lr_sched = optax.linear_schedule(0, lr, warmup_steps, transition_begin=0)\n",
    "        if optimizer == 'adam':\n",
    "            tx = optax.chain(\n",
    "                    optax.adam(learning_rate=lr_sched),\n",
    "                )\n",
    "        elif optimizer == 'sgd':\n",
    "            tx = optax.sgd(learning_rate=lr_sched, momentum=0.9)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return train_state.TrainState.create(apply_fn=model.apply, params=init_params, tx=tx)\n",
    "    \n",
    "    ts = get_train_state(optimizer, lr, init_params)\n",
    "    \n",
    "    def augmented_loss_fn(p, x_eval, dp):\n",
    "        # loss = likelihood / n_train = 1 / (2 * sigma^2) * ||y - f(x)||^2 / n_train\n",
    "        # it contains a factor 1 / n_train\n",
    "        # all other terms should be divided by n_train as well\n",
    "        avg_nll = train_loss_fn(p) \n",
    "        test_loss = test_loss_fn(p)\n",
    "        # number of params\n",
    "        P = jax.tree_util.tree_leaves(p)[0].shape[0]\n",
    "        if method == 'fsmap':\n",
    "            # logdet = 1 / 2 * log_det_g_svd(model, p, x_eval, jitter) / n_train\n",
    "            logdet = 1 / 2 * logdet_calculator.compute_log_det(p, jitter) / n_train\n",
    "        elif method == 'psmap':\n",
    "            # logdet = 1 / 2 * log_det_g_svd(model, p, x_eval, jitter) / n_train\n",
    "            logdet = 1 / 2 * logdet_calculator.compute_log_det(p, jitter) / n_train\n",
    "            logdet = jax.lax.stop_gradient(logdet)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        neg_log_p_w = neg_log_p_w_fn(p) / n_train #jnp.sum((params_flat ** 2) / (2 * (prior_stds ** 2))) / n_train\n",
    "        logdet =  temp * logdet + (1 - temp) * jax.lax.stop_gradient(logdet)\n",
    "        # return avg_nll + neg_log_p_w + logdet, (avg_nll, logdet, test_loss)\n",
    "        return avg_nll + neg_log_p_w + logdet, (avg_nll, logdet, test_loss)\n",
    "\n",
    "    grad_fn = jax.jit(jax.value_and_grad(lambda p, x_eval, dp: augmented_loss_fn(p, x_eval, dp), has_aux=True))\n",
    "    losses = []\n",
    "    logdets = []\n",
    "    avg_nlls = []\n",
    "    test_losss = []\n",
    "    def sample_dp(p, rng_key):\n",
    "        # gaussian tree\n",
    "        dp = tree_random_normal_like(rng_key, p)\n",
    "        return dp\n",
    "    @jax.jit\n",
    "    def train_step(ts, rng_key):\n",
    "        rng_key, x_eval_key = jax.random.split(rng_key)\n",
    "        rng_key, dp_key = jax.random.split(rng_key)\n",
    "        x_eval = x_eval_generator(x_eval_key)\n",
    "        dp = sample_dp(ts.params, dp_key)\n",
    "        (loss, aux), grads = grad_fn(ts.params, x_eval, dp)\n",
    "        ts = ts.apply_gradients(grads=grads)\n",
    "        return ts, loss, aux, rng_key\n",
    "    for _ in tqdm(range(int(n_step))):\n",
    "        ts, loss, aux, rng_key = train_step(ts, rng_key)\n",
    "        avg_nll, logdet, test_loss = aux\n",
    "        losses.append(loss.item())\n",
    "        logdets.append(logdet.item())\n",
    "        avg_nlls.append(avg_nll.item())\n",
    "        test_losss.append(test_loss.item())\n",
    "    losses = np.array(losses)\n",
    "    logdets = np.array(logdets)\n",
    "    avg_nlls = np.array(avg_nlls)\n",
    "    test_losss = np.array(test_losss)\n",
    "    return ts.params, losses, avg_nlls, logdets, test_losss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_hess_eig(loss_fn, p):\n",
    "    # fn: parameters pytree -> number\n",
    "    # p: parameters\n",
    "    # return average eigenvalue of hessian at p\n",
    "    p_flat, unflatten = jax.flatten_util.ravel_pytree(p)\n",
    "    def loss_fn_flat(p_flat):\n",
    "        p = unflatten(p_flat)\n",
    "        return loss_fn(p)\n",
    "    hess_fn = jax.jit(jax.hessian(loss_fn_flat), device=jax.devices('cpu')[0])\n",
    "    hess = hess_fn(p_flat)\n",
    "    eigvals = jnp.linalg.eigvalsh(hess)\n",
    "    mean_eig = eigvals.mean().item()\n",
    "    assert mean_eig > 0, 'mean eigenvalue of hessian is non-positive'\n",
    "    return mean_eig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dataset(model, in_dim, n_train, x_eval_generator, n_step, lr, optimizer, seed, method, noise_std, jitter, temp=1, result_path=None, plot=False, prior_misspecification=1, noise_misspecification=1):\n",
    "    # define log det calculator\n",
    "    x_eval = x_eval_generator(jax.random.PRNGKey(0))\n",
    "    logdet_calculator = LogDet(model, x_eval)\n",
    "    # count parameters\n",
    "    dummy_param = model.init(jax.random.PRNGKey(0), jnp.ones((1, in_dim)))\n",
    "    P = param_size(dummy_param)\n",
    "    print(f\"Number of parameters: {P}\")\n",
    "\n",
    "    rng_key = jax.random.PRNGKey(seed)\n",
    "    \n",
    "    \n",
    "    prior_stds = model.prior_stds # (p,)\n",
    "    rng_key, init_params_key = jax.random.split(rng_key)\n",
    "    w_true = model.init(init_params_key, jnp.ones((1, in_dim))) # (p,)\n",
    "    rng_key, x_train_key = jax.random.split(rng_key)\n",
    "    x_train = jax.random.uniform(x_train_key, (n_train, in_dim), minval=-1, maxval=1)\n",
    "    y_train = model.apply(w_true, x_train)\n",
    "    rng_key, noise_key = jax.random.split(rng_key)\n",
    "    noise = jax.random.normal(noise_key, y_train.shape) * noise_std\n",
    "    print('Y train mean:', jnp.mean(y_train))\n",
    "    print('Y train std:', jnp.std(y_train))\n",
    "    y_train = y_train + noise\n",
    "    \n",
    "    # misspecification\n",
    "    noise_std = noise_std / noise_misspecification\n",
    "    prior_stds = prior_stds / prior_misspecification\n",
    "    \n",
    "    # x_test = jax.random.uniform(jax.random.PRNGKey(99), (1000, in_dim), minval=-1, maxval=1)\n",
    "    x_test = jnp.linspace(-1, 1, 1000).reshape(-1, 1)\n",
    "    y_test = model.apply(w_true, x_test)\n",
    "    # zero initialization\n",
    "    init_params = jax.tree_util.tree_map(lambda x: jnp.zeros_like(x), w_true)\n",
    "    train_loss_fn = lambda p: jnp.mean((model.apply(p, x_train) - y_train) ** 2) / (2 * noise_std ** 2)\n",
    "    test_loss_fn = lambda p: jnp.mean((model.apply(p, x_test) - y_test) ** 2) / (2 * noise_std ** 2)\n",
    "    neg_log_p_w_fn = lambda p: jnp.sum((jax.flatten_util.ravel_pytree(p)[0] ** 2) / (2 * (prior_stds ** 2)))\n",
    "    train_rmse = lambda p: jnp.sqrt(jnp.mean((model.apply(p, x_train) - y_train) ** 2))\n",
    "    train_mse = lambda p: jnp.mean((model.apply(p, x_train) - y_train) ** 2)\n",
    "    test_rmse = lambda p: jnp.sqrt(jnp.mean((model.apply(p, x_test) - y_test) ** 2))\n",
    "    \n",
    "    # log training time\n",
    "    start_time = time.time()\n",
    "    params, losses, avg_nlls, logdets, test_losss = optimize(train_loss_fn, test_loss_fn, neg_log_p_w_fn, n_train, init_params, lr, n_step, rng_key, model, x_eval_generator, logdet_calculator, optimizer, jitter, method, temp)\n",
    "    end_time = time.time()\n",
    "    # train time in seconds\n",
    "    train_time = end_time - start_time\n",
    "    # plot and save losses\n",
    "    plt.figure()\n",
    "    \n",
    "    def make_subplots(arrays, labels):\n",
    "        n_plots = len(arrays)\n",
    "        fig, axs = plt.subplots(n_plots, 1, figsize=(6, 6))\n",
    "        for i, (a, label) in enumerate(zip(arrays, labels)):\n",
    "            q1, q2 = np.quantile(a, [0., 0.98])\n",
    "            axs[i].plot(a)\n",
    "            axs[i].set_ylim(q1, q2)\n",
    "            axs[i].set_xlabel('Step')\n",
    "            axs[i].set_ylabel(label)\n",
    "        plt.show()\n",
    "    \n",
    "    make_subplots([losses, logdets, avg_nlls, test_losss], ['Loss', 'Logdet', 'Train NLL', 'Test NLL'])\n",
    "    \n",
    "    # plot data and predictions\n",
    "    if plot:\n",
    "        plt.figure(figsize=(10, 8), dpi=100)\n",
    "        plt.scatter(x_train.squeeze(-1), y_train.squeeze(-1), label='Train', s=20, color='black')\n",
    "        plt.plot(x_test.squeeze(-1), y_test.squeeze(-1), label='Test', color='red')\n",
    "        plt.plot(x_test.squeeze(-1), model.apply(params, x_test).squeeze(-1), label='Predictions', color='blue')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    def measure(params, logdet_calculator):\n",
    "        neg_log_param_prior = neg_log_p_w_fn(params).item()\n",
    "        log_det = logdet_calculator.compute_log_det(params, jitter=1e-32).item()\n",
    "        train_loss = train_loss_fn(params).item()\n",
    "        measurements = {\n",
    "            'log_det': log_det,\n",
    "            'log_fs_posterior': -n_train * train_loss - neg_log_param_prior - 1 / 2 * log_det,\n",
    "            'train_rmse': train_rmse(params).item(),\n",
    "            'test_rmse': test_rmse(params).item(),\n",
    "            'hess_train_mse': avg_hess_eig(train_mse, params),\n",
    "            'log_ps_posterior': -n_train * train_loss - neg_log_param_prior,\n",
    "            'neg_log_param_prior': neg_log_param_prior,\n",
    "            'train_time': train_time,\n",
    "            'jitter': jitter,\n",
    "            'train_loss': train_loss,\n",
    "            'test_loss': test_loss_fn(params).item(),\n",
    "        }\n",
    "        print(measurements)\n",
    "        # return log_likelihood + neg_log_param_prior - 1 / 2 * log_det\n",
    "        return measurements\n",
    "    \n",
    "    # compute function space posterior \n",
    "    measurements = measure(params, logdet_calculator)\n",
    "    if result_path is not None:\n",
    "        torch.save(measurements, result_path)\n",
    "\n",
    "    return measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fourier_features(dim, num_freqs_per_dim, min_freq, max_freq):\n",
    "    \"Return a function that computes Fourier features (sin and cos) for an input of given dimension.\"\n",
    "    assert min_freq > 0 and max_freq > 0, \"min_freq and max_freq must both be positive\"\n",
    "    # num_freqs = num_freqs_per_dim ** dim\n",
    "    # create a (num_freqs, dim) matrix of frequencies on a dim-dimensional lattice\n",
    "    k = jnp.stack(jnp.meshgrid(*[jnp.linspace(min_freq, max_freq, num_freqs_per_dim)] * dim), axis=-1).reshape(-1, dim) # (num_freqs, dim)\n",
    "    def feature_fn(x):\n",
    "        # (n, dim) -> (n, 2 * num_freqs_per_dim ** dim)\n",
    "        return jnp.concatenate([jnp.sin(x @ k.T), jnp.cos(x @ k.T)], axis=-1)\n",
    "    return feature_fn\n",
    "\n",
    "def rbf_features(cs, width, height=1):\n",
    "    # RBF function\n",
    "    def rbf(x, c, width):\n",
    "        return height * jnp.exp(-(x - c) ** 2 / width)\n",
    "    def feature_fn(x):\n",
    "        feats = [rbf(x, c, width) for c in cs]\n",
    "        feats = jnp.concatenate(feats, axis=-1)\n",
    "        return feats\n",
    "    return feature_fn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fourier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FSMAP\n",
    "# data generation\n",
    "in_dim = 1\n",
    "num_freqs_per_dim = 100\n",
    "min_freq = 1 * 2 * np.pi / 2 # n 2 pi / L\n",
    "max_freq = num_freqs_per_dim * 2 * np.pi / 2\n",
    "feature_name = 'fourier1d'\n",
    "feature_fn = fourier_features(in_dim, num_freqs_per_dim, min_freq, max_freq) # 2 * num_freqs_per_dim ** dim features\n",
    "prior_stds = 10 * np.ones(2 * num_freqs_per_dim ** in_dim)\n",
    "act_fn = jax.nn.tanh\n",
    "model = NonLinearMixture(feature_fn, act_fn, prior_stds, is_orthogonal=True)\n",
    "\n",
    "# optimizer\n",
    "optimizer = 'adam'\n",
    "lr = 1e-1\n",
    "n_step = int(2500)\n",
    "\n",
    "# method\n",
    "method = 'fsmap'\n",
    "jitter = 1e-32\n",
    "def x_eval_generator(rng_key):\n",
    "    return jax.random.uniform(rng_key, shape=(10000, in_dim), minval=-1, maxval=1)\n",
    "\n",
    "# ------------------------------ #\n",
    "output_dir = f'nonlinear_mixture/weights/{feature_name}'\n",
    "skip_if_done = True\n",
    "fsmap_results = []\n",
    "\n",
    "seeds = range(3)\n",
    "n_trains = [100, 200, 400, 800, 1600, 3200, 6400]\n",
    "noise_stds = [0.1]\n",
    "\n",
    "for seed, n_train, noise_std in itertools.product(seeds, n_trains, noise_stds):\n",
    "    # mkdir if needed\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    result_path = f'{output_dir}/{method}_seed{seed}_ntrain{n_train}_noise{noise_std}.pt'\n",
    "    if os.path.exists(result_path) and skip_if_done:\n",
    "        r = torch.load(result_path)\n",
    "        print('Loaded result from ', result_path)\n",
    "    else:\n",
    "        r = run_dataset(model, in_dim, n_train, x_eval_generator, n_step, lr, optimizer, seed, method, noise_std, jitter, temp=1, result_path=result_path)\n",
    "    r['n_train'] = n_train\n",
    "    r['noise_std'] = noise_std\n",
    "    r['seed'] = seed\n",
    "    fsmap_results.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PSMAP\n",
    "# data generation\n",
    "in_dim = 1\n",
    "num_freqs_per_dim = 100\n",
    "min_freq = 1 * 2 * np.pi / 2 # n 2 pi / L\n",
    "max_freq = num_freqs_per_dim * 2 * np.pi / 2\n",
    "feature_name = 'fourier1d'\n",
    "feature_fn = fourier_features(in_dim, num_freqs_per_dim, min_freq, max_freq) # 2 * num_freqs_per_dim ** dim features\n",
    "prior_stds = 10 * np.ones(2 * num_freqs_per_dim ** in_dim)\n",
    "act_fn = jax.nn.tanh\n",
    "model = NonLinearMixture(feature_fn, act_fn, prior_stds, is_orthogonal=True)\n",
    "\n",
    "# optimizer\n",
    "optimizer = 'adam'\n",
    "lr = 1e-1\n",
    "n_step = int(2500)\n",
    "\n",
    "# method\n",
    "method = 'psmap'\n",
    "jitter = 1e-32\n",
    "def x_eval_generator(rng_key):\n",
    "    return jax.random.uniform(rng_key, shape=(10000, in_dim), minval=-1, maxval=1)\n",
    "\n",
    "# ------------------------------ #\n",
    "output_dir = f'nonlinear_mixture/weights/{feature_name}'\n",
    "skip_if_done = True\n",
    "psmap_results = []\n",
    "\n",
    "seeds = range(3)\n",
    "# n_trains = [5, 10, 20, 40, 80, 160, 320, 640, 1280, 2560]\n",
    "n_trains = [100, 200, 400, 800, 1600, 3200, 6400]\n",
    "noise_stds = [0.1]\n",
    "\n",
    "for seed, n_train, noise_std in itertools.product(seeds, n_trains, noise_stds):\n",
    "    # mkdir if needed\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    result_path = f'{output_dir}/{method}_seed{seed}_ntrain{n_train}_noise{noise_std}.pt'\n",
    "    if os.path.exists(result_path) and skip_if_done and 'hess_train_mse' in torch.load(result_path):\n",
    "        r = torch.load(result_path)\n",
    "        print('Loaded result from ', result_path)\n",
    "    else:\n",
    "        r = run_dataset(model, in_dim, n_train, x_eval_generator, n_step, lr, optimizer, seed, method, noise_std, jitter, temp=1, result_path=result_path)\n",
    "    r['n_train'] = n_train\n",
    "    r['noise_std'] = noise_std\n",
    "    r['seed'] = seed\n",
    "    psmap_results.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert fsmap results to pandas dataframe\n",
    "import pandas as pd\n",
    "df_fsmap = pd.DataFrame(fsmap_results)\n",
    "df_psmap = pd.DataFrame(psmap_results)\n",
    "df_fsmap['method'] = 'FS-MAP'\n",
    "df_psmap['method'] = 'PS-MAP'\n",
    "\n",
    "df = pd.concat([df_fsmap, df_psmap])\n",
    "# sort by n_train and method\n",
    "df = df.sort_values(by=['n_train', 'method'])\n",
    "# add a column for generalization gap\n",
    "df['generalization_gap'] = df['test_rmse'] - df['train_rmse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker\n",
    "from matplotlib.ticker import ScalarFormatter, NullFormatter\n",
    "\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"paper\", font_scale=3, rc={\"lines.linewidth\": 2})\n",
    "sns.set_palette(\"Set1\")\n",
    "\n",
    "os.makedirs(f'nonlinear_mixture/plots/{feature_name}', exist_ok=True)\n",
    "\n",
    "# plt.rcParams['ytick.labelsize'] = 'x-small'\n",
    "    \n",
    "# Create a new DataFrame where each row is a trial for a specific method and 'n_train' value\n",
    "df_diff = pd.DataFrame()\n",
    "\n",
    "df_fs_map = df[df.method == 'FS-MAP']\n",
    "df_ps_map = df[df.method == 'PS-MAP']\n",
    "\n",
    "# Make sure the trials align between FS-MAP and PS-MAP\n",
    "assert all(df_fs_map.n_train.values == df_ps_map.n_train.values), 'n_train values do not align between FS-MAP and PS-MAP'\n",
    "\n",
    "df_diff['n_train'] = df_fs_map.n_train\n",
    "df_diff['difference'] = (df_fs_map.log_fs_posterior.values - df_ps_map.log_fs_posterior.values)\n",
    "# Plot the differences using sns.lineplot\n",
    "plt.figure(figsize=(6,5), dpi=200)\n",
    "plot = sns.lineplot(data=df_diff, x='n_train', y='difference', ci='sd', errorbar='sd', marker='o', markersize=5)\n",
    "plot.set_xscale('log')\n",
    "plot.set_yscale('log')\n",
    "plot.yaxis.set_minor_formatter(NullFormatter())\n",
    "plot.set_yticks([1e3, 6e3])\n",
    "plt.xlabel('# Train samples')\n",
    "plt.ylabel('')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'nonlinear_mixture/plots/{feature_name}/log_fs_posterior.pdf')\n",
    "plt.savefig(f'nonlinear_mixture/plots/{feature_name}/log_fs_posterior.png')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,5), dpi=200)\n",
    "sns.lineplot(x='n_train', y='hess_train_mse', data=df[df.method == 'PS-MAP'], linestyle='--', label='PS-MAP', color='C1', errorbar='sd', marker='o', markersize=5)\n",
    "plot = sns.lineplot(x='n_train', y='hess_train_mse', data=df[df.method == 'FS-MAP'], linestyle='--', label='FS-MAP', color='C0', errorbar='sd', marker='o', markersize=5)\n",
    "plt.xlabel('# Train samples')\n",
    "plt.ylabel('')\n",
    "plot.set_xscale('log')\n",
    "plot.set_yscale('log')\n",
    "plot.yaxis.set_minor_formatter(NullFormatter())\n",
    "plot.set_yticks([1e-3, 1e-4])\n",
    "plt.legend(loc='upper right', prop={'size': 24})\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'nonlinear_mixture/plots/{feature_name}/hess_train_mse.pdf')\n",
    "plt.savefig(f'nonlinear_mixture/plots/{feature_name}/hess_train_mse.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6,5), dpi=200)\n",
    "sns.lineplot(x='n_train', y='test_rmse', data=df[df.method == 'PS-MAP'], linestyle='-', label='PS-MAP', color='C1', errorbar='sd', marker='o', markersize=5)\n",
    "sns.lineplot(x='n_train', y='test_rmse', data=df[df.method == 'FS-MAP'], linestyle='-', label='FS-MAP', color='C0', errorbar='sd', marker='o', markersize=5)\n",
    "plt.xlabel('# Train samples')\n",
    "plt.ylabel('')\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.legend([],[], frameon=False)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'nonlinear_mixture/plots/{feature_name}/test_rmse.pdf')\n",
    "plt.savefig(f'nonlinear_mixture/plots/{feature_name}/test_rmse.png')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,5), dpi=200)\n",
    "sns.lineplot(x='n_train', y='train_rmse', data=df[df.method == 'PS-MAP'], linestyle='--', label='PS-MAP', color='C1', errorbar='sd', marker='o', markersize=5)\n",
    "plot = sns.lineplot(x='n_train', y='train_rmse', data=df[df.method == 'FS-MAP'], linestyle='--', label='FS-MAP', color='C0', errorbar='sd', marker='o', markersize=5)\n",
    "plt.xlabel('# Train samples')\n",
    "plt.ylabel('')\n",
    "plot.set_xscale('log')\n",
    "plot.set_yscale('log')\n",
    "plot.yaxis.set_minor_formatter(NullFormatter())\n",
    "plot.set_yticks([1e-1, 3e-2])\n",
    "# remove legend\n",
    "plt.legend([],[], frameon=False)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'nonlinear_mixture/plots/{feature_name}/train_rmse.pdf')\n",
    "plt.savefig(f'nonlinear_mixture/plots/{feature_name}/train_rmse.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('fspace': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9436057e92285046d415c34e216bd357b01decd87fa7e06f42744a4b160880c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
