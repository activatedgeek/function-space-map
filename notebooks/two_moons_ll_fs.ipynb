{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set(font_scale=1.5, style='whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fspace.datasets import get_dataset, get_dataset_normalization\n",
    "from fspace.datasets.two_moons import get_twomoons_ctx\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_data, _, test_data = get_dataset('twomoons', random_state=137)\n",
    "ctx_data = get_twomoons_ctx(n_samples=10000, random_state=137, normalize=get_dataset_normalization('twomoons'))\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=10, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=1000)\n",
    "ctx_loader = DataLoader(ctx_data, batch_size=10)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "ax.scatter(train_data.X[:, 0], train_data.X[:, 1], c=train_data.y, cmap=sns.color_palette('Accent', as_cmap=True))\n",
    "# ax.scatter(ctx_data.X[:, 0], ctx_data.X[:, 1], color='black', alpha=0.25, label=r'$\\mathbf{X}_C$')\n",
    "ax.legend()\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import optax\n",
    "import jax.numpy as jnp\n",
    "import distrax\n",
    "from tqdm.auto import tqdm\n",
    "import logging\n",
    "from fspace.nn import create_model\n",
    "from fspace.utils.training import TrainState, eval_classifier\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def train_step_fn(prior_params, state, b_X, b_Y, b_X_ctx, f_prior_std, jitter=1e-4):\n",
    "    '''\n",
    "    NOTE: Prior means for all parameters is assumed to be zero.\n",
    "    '''\n",
    "    B = b_X.shape[0]\n",
    "\n",
    "    def loss_fn(params, **extra_vars):\n",
    "        b_X_in = b_X if b_X_ctx is None else jnp.concatenate([b_X, b_X_ctx], axis=0)\n",
    "\n",
    "        b_logits, new_state = state.apply_fn({ 'params': params, **extra_vars }, b_X_in,\n",
    "                                             mutable=['batch_stats'], train=True)\n",
    "\n",
    "        loss = jnp.mean(optax.softmax_cross_entropy_with_integer_labels(b_logits[:B], b_Y))\n",
    "\n",
    "        h_X = state.apply_fn({ 'params': prior_params, **extra_vars }, b_X_in,\n",
    "                                                   mutable=['batch_stats', 'intermediates'], train=True)[1]['intermediates']['features'][0]\n",
    "\n",
    "        f_h_cov = jnp.matmul(h_X * f_prior_std**2, h_X.T)\n",
    "        f_h_cov = f_h_cov + f_prior_std**2 * jnp.ones_like(f_h_cov) + f_prior_std**2 * jnp.eye(f_h_cov.shape[0])\n",
    "        f_dist = distrax.MultivariateNormalFullCovariance(\n",
    "            loc=jnp.zeros(f_h_cov.shape[0]), covariance_matrix=f_h_cov)\n",
    "\n",
    "        reg_loss = - jnp.sum(f_dist.log_prob(b_logits.T))\n",
    "\n",
    "        total_loss = loss + reg_loss\n",
    "\n",
    "        return total_loss, (new_state, loss, reg_loss)\n",
    "\n",
    "    (_, (new_state, loss, reg_loss)), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params, **state.extra_vars)\n",
    "\n",
    "    final_state = state.apply_gradients(grads=grads, **new_state)\n",
    "\n",
    "    step_metrics = {\n",
    "        'batch_loss': loss,\n",
    "        'batch_reg_loss': reg_loss,\n",
    "    }\n",
    "\n",
    "    return final_state, step_metrics\n",
    "\n",
    "\n",
    "def train_model(prior_sample, state, loader, ctx_loader=None, log_dir=None, epoch=None):\n",
    "    ctx_iter = iter(ctx_loader or [[None, None]])\n",
    "\n",
    "    for i, (X, Y) in enumerate(loader):\n",
    "        X, Y = X.numpy(), Y.numpy()\n",
    "        try:\n",
    "            X_ctx, _ = next(ctx_iter)\n",
    "        except StopIteration:\n",
    "            ctx_iter = iter(ctx_loader or [[None, None]])\n",
    "            X_ctx, _ = next(ctx_iter)\n",
    "        if X_ctx is not None:\n",
    "            X_ctx = X_ctx.numpy()\n",
    "\n",
    "        state, step_metrics = train_step_fn(prior_sample, state, X, Y, X_ctx, f_prior_std)\n",
    "\n",
    "        step_metrics = { k: v.item() for k, v in step_metrics.items() }\n",
    "        print(step_metrics)\n",
    "\n",
    "    return state\n",
    "\n",
    "weight_decay = 0.\n",
    "alpha = 0.0\n",
    "lr = 1e-3\n",
    "epochs = 10000\n",
    "f_prior_std = 10000.\n",
    "\n",
    "rng = jax.random.PRNGKey(42)\n",
    "\n",
    "rng, model_rng = jax.random.split(rng)\n",
    "model, init_params, init_vars = create_model(model_rng, 'mlp200', train_data[0][0].numpy()[None, ...], num_classes=train_data.n_classes)\n",
    "\n",
    "optimizer = optax.adamw(learning_rate=lr, weight_decay=weight_decay)\n",
    "\n",
    "train_state = TrainState.create(\n",
    "    apply_fn=model.apply,\n",
    "    params=init_params,\n",
    "    **init_vars,\n",
    "    tx=optimizer)\n",
    "\n",
    "for e in tqdm(range(epochs)):\n",
    "    rng, model_rng = jax.random.split(rng)\n",
    "    _, reinit_params, _ = create_model(model_rng, 'mlp200', train_data[0][0].numpy()[None, ...], num_classes=train_data.n_classes)\n",
    "\n",
    "    train_state = train_model(reinit_params, train_state, train_loader, ctx_loader=ctx_loader, epoch=e)\n",
    "\n",
    "    if e % 100 == 0:\n",
    "        print(eval_classifier(train_state, train_loader))\n",
    "\n",
    "eval_classifier(train_state, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def _forward(X):\n",
    "    return train_state.apply_fn({ 'params': train_state.params, **train_state.extra_vars}, X, mutable=False, train=False)\n",
    "\n",
    "test_logits = []\n",
    "for X, Y in tqdm(test_loader, leave=False):\n",
    "    X, Y = X.numpy(), Y.numpy()\n",
    "\n",
    "    test_logits.append(_forward(X))\n",
    "\n",
    "test_p = jax.nn.softmax(jnp.concatenate(test_logits), axis=-1)\n",
    "\n",
    "test_p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "plt_xy = test_data.X.reshape(100, 100, -1)\n",
    "plt_z = test_p[:, 0].reshape(100, 100)\n",
    "\n",
    "contourf_ = ax.contourf(plt_xy[..., 0], plt_xy[..., 1], plt_z, vmin=0., vmax=1., cmap='viridis')\n",
    "ax.scatter(train_data.X[..., 0], train_data.X[..., 1], c=train_data.y, cmap=sns.color_palette('tab20', as_cmap=True))\n",
    "# ax.scatter(ctx_data.X[:, 0], ctx_data.X[:, 1], color='black', alpha=0.1)\n",
    "\n",
    "fig.colorbar(contourf_)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cf78c7e6f2efe72a4ab2b9c73f062685208ec2c699b65763514c17bcea1347f8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
