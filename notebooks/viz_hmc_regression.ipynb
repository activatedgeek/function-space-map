{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import blackjax\n",
    "\n",
    "sns.set(font_scale=2, style='whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_raw_data = pd.read_csv('snelson.csv')\n",
    "x_all, y_all = np.atleast_2d(_raw_data.x.values).T, np.atleast_2d(_raw_data.y.values).T\n",
    "# x_all = (x_all - np.mean(x_all, axis=0, keepdims=True)) / np.std(x_all, axis=0, keepdims=True)\n",
    "# y_all = (y_all - np.mean(y_all, axis=0, keepdims=True)) / np.std(y_all, axis=0, keepdims=True)\n",
    "x_all.shape, y_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.linspace(x_all.min() - 1., x_all.max() + 1., 500)\n",
    "x_test = np.atleast_2d(x_test).T\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 4))\n",
    "\n",
    "ax.scatter(x_all, y_all)\n",
    "\n",
    "ax.set(xlabel='x', ylabel='y')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import linen as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    out_size: int\n",
    "    H: int = 100\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        out = nn.Sequential([\n",
    "            nn.Dense(features=self.H),\n",
    "            nn.tanh,\n",
    "            nn.Dense(features=self.H),\n",
    "            nn.tanh,\n",
    "            nn.Dense(features=self.out_size),\n",
    "        ])(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_key = jax.random.PRNGKey(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(out_size=1)\n",
    "rng_key, init_params_key = jax.random.split(rng_key)\n",
    "init_params = jax.jit(model.init)(init_params_key, jnp.ones((1, 1)))\n",
    "n_params = sum([len(jnp.ravel(p)) for p in jax.tree_util.tree_flatten(init_params)[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import distrax\n",
    "\n",
    "\n",
    "def logprior_fn(params):\n",
    "    leaves, _ = jax.tree_util.tree_flatten(params)\n",
    "    flat_params = jnp.concatenate([jnp.ravel(p) for p in leaves])\n",
    "    lik = distrax.Normal(0., 1.).log_prob(flat_params)\n",
    "    return jnp.sum(lik)\n",
    "\n",
    "def loglikelihood_fn(params, X, Y, model):\n",
    "    f = model.apply(params, X)\n",
    "    lik = distrax.Normal(f, 1.).log_prob(Y)\n",
    "    return jnp.sum(lik)\n",
    "\n",
    "def logprob_fn(params, X, Y, model):\n",
    "    return loglikelihood_fn(params, X, Y, model) + logprior_fn(params)\n",
    "\n",
    "def inference_loop(rng_key, kernel, initial_state, num_samples):\n",
    "    @jax.jit\n",
    "    def one_step(state, rng_key):\n",
    "        state, _ = kernel(rng_key, state)\n",
    "        return state, state\n",
    "\n",
    "    keys = jax.random.split(rng_key, num_samples)\n",
    "    _, states = jax.lax.scan(one_step, initial_state, keys)\n",
    "\n",
    "    return states\n",
    "\n",
    "logposterior_fn = partial(logprob_fn, X=x_all, Y=y_all, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_warmup = 500\n",
    "adapt = blackjax.window_adaptation(blackjax.nuts, logposterior_fn, n_warmup)\n",
    "rng_key, warmup_key = jax.random.split(rng_key)\n",
    "final_state, kernel, _ = adapt.run(warmup_key, init_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 100\n",
    "rng_key, inference_key = jax.random.split(rng_key)\n",
    "states = inference_loop(inference_key, kernel, final_state, num_samples)\n",
    "samples = states.position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, samples, X):\n",
    "    vmap = jax.vmap(model.apply, in_axes=(0, None), out_axes=0)\n",
    "    predictions = vmap(samples, X)\n",
    "    return predictions.squeeze(-1)\n",
    "\n",
    "predictions = get_predictions(model, samples, x_test)\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9,6))\n",
    "\n",
    "y_test = np.mean(predictions, axis=0)\n",
    "y_test_std = np.std(predictions, axis=0)\n",
    "ax.plot(x_test[..., 0], y_test, c='red', label='NUTS Mean')\n",
    "ax.fill_between(x_test[..., 0], y_test - 2 * y_test_std, y_test + 2 * y_test_std, color='red', alpha=.2)\n",
    "\n",
    "ax.scatter(x_all, y_all, label='Train Data')\n",
    "\n",
    "ax.set(xlabel='x', ylabel='y', ylim=[-3,3])\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_key = jax.random.PRNGKey(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(out_size=1)\n",
    "rng_key, init_params_key = jax.random.split(rng_key)\n",
    "init_params = jax.jit(model.init)(init_params_key, jnp.ones((1, 1)))\n",
    "n_params = sum([len(jnp.ravel(p)) for p in jax.tree_util.tree_flatten(init_params)[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax.training import train_state\n",
    "import optax\n",
    "\n",
    "lr = 3e-2\n",
    "momentum = .9\n",
    "tx = optax.sgd(lr, momentum=momentum)\n",
    "ts = train_state.TrainState.create(apply_fn=model.apply, params=init_params, tx=tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_loss_fn(params, X, Y, model, lmbda=1e-2):\n",
    "    y_pred = model.apply(params, X)\n",
    "    loss = jnp.mean((Y - y_pred)**2)\n",
    "    \n",
    "    leaves, _ = jax.tree_util.tree_flatten(params)\n",
    "    reg = jnp.sum(jnp.array([jnp.sum(jnp.ravel(p)**2) for p in leaves]))\n",
    "    return loss + .5 * lmbda * reg\n",
    "\n",
    "loss_fn = partial(l2_loss_fn, X=x_all, Y=y_all, model=model, lmbda=1e-3)\n",
    "grad_fn = jax.value_and_grad(loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: lax scan?\n",
    "for e in tqdm(range(3000)):\n",
    "    _, grads = grad_fn(ts.params)\n",
    "    ts = ts.apply_gradients(grads=grads)\n",
    "\n",
    "    if (e + 1) % 500 == 0:\n",
    "        y_pred = model.apply(ts.params, x_all)\n",
    "        loss = jnp.mean((y_all - y_pred)**2)\n",
    "        print(f'Loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9,6))\n",
    "\n",
    "y_test = model.apply(ts.params, x_test)[..., 0]\n",
    "ax.plot(x_test[..., 0], y_test, c='black', label='GD MAP')\n",
    "\n",
    "y_test = np.mean(predictions, axis=0)\n",
    "y_test_std = np.std(predictions, axis=0)\n",
    "ax.plot(x_test[..., 0], y_test, c='red', label='NUTS Mean')\n",
    "ax.fill_between(x_test[..., 0], y_test - 2 * y_test_std, y_test + 2 * y_test_std, color='red', alpha=.2)\n",
    "\n",
    "ax.scatter(x_all, y_all, label='Train Data', alpha=.5)\n",
    "\n",
    "ax.set(xlabel='x', ylabel='y', ylim=[-3,3])\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('fspace')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cf78c7e6f2efe72a4ab2b9c73f062685208ec2c699b65763514c17bcea1347f8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
